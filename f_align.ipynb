{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c265255c-06b9-4002-b42c-05946a05d54a","_uuid":"1ac11d47-c456-40ee-a429-ce70b1475690","collapsed":false,"execution":{"iopub.execute_input":"2024-09-09T10:06:06.682907Z","iopub.status.busy":"2024-09-09T10:06:06.682594Z","iopub.status.idle":"2024-09-09T10:06:08.906149Z","shell.execute_reply":"2024-09-09T10:06:08.904924Z","shell.execute_reply.started":"2024-09-09T10:06:06.682871Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["!git clone https://github.com/isi-nlp/uroman.git"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"075a2fc1-4f48-49f4-85b5-2aa6d81b21af","_uuid":"cb142bf1-04b4-4ba3-90fa-e8fc6b5e811a","collapsed":false,"execution":{"iopub.execute_input":"2024-09-09T10:06:08.909192Z","iopub.status.busy":"2024-09-09T10:06:08.908812Z","iopub.status.idle":"2024-09-09T10:06:28.682165Z","shell.execute_reply":"2024-09-09T10:06:28.681090Z","shell.execute_reply.started":"2024-09-09T10:06:08.909147Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["!pip install sox dataclasses"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"9050c0e1-e8da-45e3-8a10-79909e5173ea","_uuid":"e633d084-f57a-41ef-8275-8fc1c48da5da","collapsed":false,"execution":{"iopub.execute_input":"2024-09-09T10:06:28.684164Z","iopub.status.busy":"2024-09-09T10:06:28.683748Z","iopub.status.idle":"2024-09-09T10:06:29.733665Z","shell.execute_reply":"2024-09-09T10:06:29.732441Z","shell.execute_reply.started":"2024-09-09T10:06:28.684118Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["!ln -s /kaggle/working/uroman/uroman/data /kaggle/working/uroman/data"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"441bdd9b-00f8-4895-bbff-49f8179bca31","_uuid":"b2eb9027-6650-4758-833b-bcb4e6b6b322","collapsed":false,"execution":{"iopub.execute_input":"2024-09-09T10:06:29.737773Z","iopub.status.busy":"2024-09-09T10:06:29.737306Z","iopub.status.idle":"2024-09-09T10:08:23.197182Z","shell.execute_reply":"2024-09-09T10:08:23.196170Z","shell.execute_reply.started":"2024-09-09T10:06:29.737724Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["!conda install conda-forge::sox -y"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"77e2ed71-edcf-443e-b804-b5afe41fc9bf","_uuid":"96d77fc9-685c-4303-ac6b-fc672ddb86b8","collapsed":false,"execution":{"iopub.execute_input":"2024-09-09T10:08:23.199309Z","iopub.status.busy":"2024-09-09T10:08:23.198956Z","iopub.status.idle":"2024-09-09T10:08:23.220965Z","shell.execute_reply":"2024-09-09T10:08:23.220108Z","shell.execute_reply.started":"2024-09-09T10:08:23.199271Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["%%writefile align.py\n","import os\n","import torch\n","import torchaudio\n","import sox\n","import json\n","import argparse\n","\n","import os\n","import re\n","\n","\n","colon = \":\"\n","comma = \",\"\n","exclamation_mark = \"!\"\n","period = re.escape(\".\")\n","question_mark = re.escape(\"?\")\n","semicolon = \";\"\n","\n","left_curly_bracket = \"{\"\n","right_curly_bracket = \"}\"\n","quotation_mark = '\"'\n","\n","basic_punc = (\n","    period\n","    + question_mark\n","    + comma\n","    + colon\n","    + exclamation_mark\n","    + left_curly_bracket\n","    + right_curly_bracket\n",")\n","\n","# General punc unicode block (0x2000-0x206F)\n","zero_width_space = r\"\\u200B\"\n","zero_width_nonjoiner = r\"\\u200C\"\n","left_to_right_mark = r\"\\u200E\"\n","right_to_left_mark = r\"\\u200F\"\n","left_to_right_embedding = r\"\\u202A\"\n","pop_directional_formatting = r\"\\u202C\"\n","\n","# Here are some commonly ill-typed versions of apostrophe\n","right_single_quotation_mark = r\"\\u2019\"\n","left_single_quotation_mark = r\"\\u2018\"\n","\n","# Language specific definitions\n","# Spanish\n","inverted_exclamation_mark = r\"\\u00A1\"\n","inverted_question_mark = r\"\\u00BF\"\n","\n","\n","# Hindi\n","hindi_danda = u\"\\u0964\"\n","\n","# Egyptian Arabic\n","# arabic_percent = r\"\\u066A\"\n","arabic_comma = r\"\\u060C\"\n","arabic_question_mark = r\"\\u061F\"\n","arabic_semicolon = r\"\\u061B\"\n","arabic_diacritics = r\"\\u064B-\\u0652\"\n","\n","\n","arabic_subscript_alef_and_inverted_damma = r\"\\u0656-\\u0657\"\n","\n","\n","# Chinese\n","full_stop = r\"\\u3002\"\n","full_comma = r\"\\uFF0C\"\n","full_exclamation_mark = r\"\\uFF01\"\n","full_question_mark = r\"\\uFF1F\"\n","full_semicolon = r\"\\uFF1B\"\n","full_colon = r\"\\uFF1A\"\n","full_parentheses = r\"\\uFF08\\uFF09\"\n","quotation_mark_horizontal = r\"\\u300C-\\u300F\"\n","quotation_mark_vertical = r\"\\uFF41-\\uFF44\"\n","title_marks = r\"\\u3008-\\u300B\"\n","wavy_low_line = r\"\\uFE4F\"\n","ellipsis = r\"\\u22EF\"\n","enumeration_comma = r\"\\u3001\"\n","hyphenation_point = r\"\\u2027\"\n","forward_slash = r\"\\uFF0F\"\n","wavy_dash = r\"\\uFF5E\"\n","box_drawings_light_horizontal = r\"\\u2500\"\n","fullwidth_low_line = r\"\\uFF3F\"\n","chinese_punc = (\n","    full_stop\n","    + full_comma\n","    + full_exclamation_mark\n","    + full_question_mark\n","    + full_semicolon\n","    + full_colon\n","    + full_parentheses\n","    + quotation_mark_horizontal\n","    + quotation_mark_vertical\n","    + title_marks\n","    + wavy_low_line\n","    + ellipsis\n","    + enumeration_comma\n","    + hyphenation_point\n","    + forward_slash\n","    + wavy_dash\n","    + box_drawings_light_horizontal\n","    + fullwidth_low_line\n",")\n","\n","# Armenian\n","armenian_apostrophe = r\"\\u055A\"\n","emphasis_mark = r\"\\u055B\"\n","exclamation_mark = r\"\\u055C\"\n","armenian_comma = r\"\\u055D\"\n","armenian_question_mark = r\"\\u055E\"\n","abbreviation_mark = r\"\\u055F\"\n","armenian_full_stop = r\"\\u0589\"\n","armenian_punc = (\n","    armenian_apostrophe\n","    + emphasis_mark\n","    + exclamation_mark\n","    + armenian_comma\n","    + armenian_question_mark\n","    + abbreviation_mark\n","    + armenian_full_stop\n",")\n","\n","lesser_than_symbol = r\"&lt;\"\n","greater_than_symbol = r\"&gt;\"\n","\n","lesser_than_sign = r\"\\u003c\"\n","greater_than_sign = r\"\\u003e\"\n","\n","nbsp_written_form = r\"&nbsp\"\n","\n","# Quotation marks\n","left_double_quotes = r\"\\u201c\"\n","right_double_quotes = r\"\\u201d\"\n","left_double_angle = r\"\\u00ab\"\n","right_double_angle = r\"\\u00bb\"\n","left_single_angle = r\"\\u2039\"\n","right_single_angle = r\"\\u203a\"\n","low_double_quotes = r\"\\u201e\"\n","low_single_quotes = r\"\\u201a\"\n","high_double_quotes = r\"\\u201f\"\n","high_single_quotes = r\"\\u201b\"\n","\n","all_punct_quotes = (\n","    left_double_quotes\n","    + right_double_quotes\n","    + left_double_angle\n","    + right_double_angle\n","    + left_single_angle\n","    + right_single_angle\n","    + low_double_quotes\n","    + low_single_quotes\n","    + high_double_quotes\n","    + high_single_quotes\n","    + right_single_quotation_mark\n","    + left_single_quotation_mark\n",")\n","mapping_quotes = (\n","    \"[\"\n","    + high_single_quotes\n","    + right_single_quotation_mark\n","    + left_single_quotation_mark\n","    + \"]\"\n",")\n","\n","\n","# Digits\n","\n","english_digits = r\"\\u0030-\\u0039\"\n","bengali_digits = r\"\\u09e6-\\u09ef\"\n","khmer_digits = r\"\\u17e0-\\u17e9\"\n","devanagari_digits = r\"\\u0966-\\u096f\"\n","oriya_digits = r\"\\u0b66-\\u0b6f\"\n","extended_arabic_indic_digits = r\"\\u06f0-\\u06f9\"\n","kayah_li_digits = r\"\\ua900-\\ua909\"\n","fullwidth_digits = r\"\\uff10-\\uff19\"\n","malayam_digits = r\"\\u0d66-\\u0d6f\"\n","myanmar_digits = r\"\\u1040-\\u1049\"\n","roman_numeral = r\"\\u2170-\\u2179\"\n","nominal_digit_shapes = r\"\\u206f\"\n","\n","# Load punctuations from MMS-lab data\n","with open(\"/kaggle/working/punctuations.lst\", \"r\") as punc_f:\n","    punc_list = punc_f.readlines()\n","\n","punct_pattern = r\"\"\n","for punc in punc_list:\n","    # the first character in the tab separated line is the punc to be removed\n","    punct_pattern += re.escape(punc.split(\"\\t\")[0])\n","\n","shared_digits = (\n","    english_digits\n","    + bengali_digits\n","    + khmer_digits\n","    + devanagari_digits\n","    + oriya_digits\n","    + extended_arabic_indic_digits\n","    + kayah_li_digits\n","    + fullwidth_digits\n","    + malayam_digits\n","    + myanmar_digits\n","    + roman_numeral\n","    + nominal_digit_shapes\n",")\n","\n","shared_punc_list = (\n","    basic_punc\n","    + all_punct_quotes\n","    + greater_than_sign\n","    + lesser_than_sign\n","    + inverted_question_mark\n","    + full_stop\n","    + semicolon\n","    + armenian_punc\n","    + inverted_exclamation_mark\n","    + arabic_comma\n","    + enumeration_comma\n","    + hindi_danda\n","    + quotation_mark\n","    + arabic_semicolon\n","    + arabic_question_mark\n","    + chinese_punc\n","    + punct_pattern\n","\n",")\n","\n","shared_mappping = {\n","    lesser_than_symbol: \"\",\n","    greater_than_symbol: \"\",\n","    nbsp_written_form: \"\",\n","    r\"(\\S+)\" + mapping_quotes + r\"(\\S+)\": r\"\\1'\\2\",\n","}\n","\n","shared_deletion_list = (\n","    left_to_right_mark\n","    + zero_width_nonjoiner\n","    + arabic_subscript_alef_and_inverted_damma\n","    + zero_width_space\n","    + arabic_diacritics\n","    + pop_directional_formatting\n","    + right_to_left_mark\n","    + left_to_right_embedding\n",")\n","\n","norm_config = {\n","    \"*\": {\n","        \"lower_case\": True,\n","        \"punc_set\": shared_punc_list,\n","        \"del_set\": shared_deletion_list,\n","        \"mapping\": shared_mappping,\n","        \"digit_set\": shared_digits,\n","        \"unicode_norm\": \"NFKC\",\n","        \"rm_diacritics\" : False,\n","    }\n","}\n","\n","#=============== Mongolian ===============#\n","\n","norm_config[\"mon\"] = norm_config[\"*\"].copy()\n","# add soft hyphen to punc list to match with fleurs\n","norm_config[\"mon\"][\"del_set\"] += r\"\\u00AD\"\n","\n","norm_config[\"khk\"] = norm_config[\"mon\"].copy()\n","\n","#=============== Hebrew ===============#\n","\n","norm_config[\"heb\"] = norm_config[\"*\"].copy()\n","# add \"HEBREW POINT\" symbols to match with fleurs\n","norm_config[\"heb\"][\"del_set\"] += r\"\\u05B0-\\u05BF\\u05C0-\\u05CF\"\n","\n","#=============== Thai ===============#\n","\n","norm_config[\"tha\"] = norm_config[\"*\"].copy()\n","# add \"Zero width joiner\" symbols to match with fleurs\n","norm_config[\"tha\"][\"punc_set\"] += r\"\\u200D\"\n","\n","#=============== Arabic ===============#\n","norm_config[\"ara\"] = norm_config[\"*\"].copy()\n","norm_config[\"ara\"][\"mapping\"][\"ٱ\"] = \"ا\"\n","norm_config[\"arb\"] = norm_config[\"ara\"].copy()\n","\n","#=============== Javanese ===============#\n","norm_config[\"jav\"] = norm_config[\"*\"].copy()\n","norm_config[\"jav\"][\"rm_diacritics\"] = True\n","\n","import json\n","import re\n","import unicodedata\n","\n","def text_normalize(text, iso_code, lower_case=True, remove_numbers=True, remove_brackets=False):\n","\n","    \"\"\"Given a text, normalize it by changing to lower case, removing punctuations, removing words that only contain digits and removing extra spaces\n","\n","    Args:\n","        text : The string to be normalized\n","        iso_code :\n","        remove_numbers : Boolean flag to specify if words containing only digits should be removed\n","\n","    Returns:\n","        normalized_text : the string after all normalization\n","\n","    \"\"\"\n","\n","    config = norm_config.get(iso_code, norm_config[\"*\"])\n","\n","    for field in [\"lower_case\", \"punc_set\",\"del_set\", \"mapping\", \"digit_set\", \"unicode_norm\"]:\n","        if field not in config:\n","            config[field] = norm_config[\"*\"][field]\n","\n","\n","    text = unicodedata.normalize(config[\"unicode_norm\"], text)\n","\n","    # Convert to lower case\n","\n","    if config[\"lower_case\"] and lower_case:\n","        text = text.lower()\n","\n","    # brackets\n","\n","    # always text inside brackets with numbers in them. Usually corresponds to \"(Sam 23:17)\"\n","    text = re.sub(r\"\\([^\\)]*\\d[^\\)]*\\)\", \" \", text)\n","    if remove_brackets:\n","        text = re.sub(r\"\\([^\\)]*\\)\", \" \", text)\n","\n","    # Apply mappings\n","\n","    for old, new in config[\"mapping\"].items():\n","        text = re.sub(old, new, text)\n","\n","    # Replace punctutations with space\n","\n","    punct_pattern = r\"[\" + config[\"punc_set\"]\n","\n","    punct_pattern += \"]\"\n","\n","    normalized_text = re.sub(punct_pattern, \" \", text)\n","\n","    # remove characters in delete list\n","\n","    delete_patten = r\"[\" + config[\"del_set\"] + \"]\"\n","\n","    normalized_text = re.sub(delete_patten, \"\", normalized_text)\n","\n","    # Remove words containing only digits\n","    # We check for 3 cases  a)text starts with a number b) a number is present somewhere in the middle of the text c) the text ends with a number\n","    # For each case we use lookaround regex pattern to see if the digit pattern in preceded and followed by whitespaces, only then we replace the numbers with space\n","    # The lookaround enables overlapping pattern matches to be replaced\n","\n","    if remove_numbers:\n","\n","        digits_pattern = \"[\" + config[\"digit_set\"]\n","\n","        digits_pattern += \"]+\"\n","\n","        complete_digit_pattern = (\n","            r\"^\"\n","            + digits_pattern\n","            + \"(?=\\s)|(?<=\\s)\"\n","            + digits_pattern\n","            + \"(?=\\s)|(?<=\\s)\"\n","            + digits_pattern\n","            + \"$\"\n","        )\n","\n","        normalized_text = re.sub(complete_digit_pattern, \" \", normalized_text)\n","\n","    if config[\"rm_diacritics\"]:\n","        from unidecode import unidecode\n","        normalized_text = unidecode(normalized_text)\n","\n","    # Remove extra spaces\n","    normalized_text = re.sub(r\"\\s+\", \" \", normalized_text).strip()\n","\n","    return normalized_text\n","\n","import re\n","import os\n","import torch\n","import tempfile\n","import math\n","from dataclasses import dataclass\n","from torchaudio.models import wav2vec2_model\n","\n","# iso codes with specialized rules in uroman\n","special_isos_uroman = \"ara, bel, bul, deu, ell, eng, fas, grc, ell, eng, heb, kaz, kir, lav, lit, mkd, mkd2, oss, pnt, pus, rus, srp, srp2, tur, uig, ukr, yid\".split(\",\")\n","special_isos_uroman = [i.strip() for i in special_isos_uroman]\n","\n","def normalize_uroman(text):\n","    text = text.lower()\n","    text = re.sub(\"([^a-z' ])\", \" \", text)\n","    text = re.sub(' +', ' ', text)\n","    return text.strip()\n","\n","\n","def get_uroman_tokens(norm_transcripts, uroman_root_dir, iso = None):\n","    tf = tempfile.NamedTemporaryFile()\n","    tf2 = tempfile.NamedTemporaryFile()\n","    with open(tf.name, \"w\") as f:\n","        for t in norm_transcripts:\n","            f.write(t + \"\\n\")\n","\n","    assert os.path.exists(f\"{uroman_root_dir}/uroman.pl\"), \"uroman not found\"\n","    cmd = f\"perl {uroman_root_dir}/uroman.pl\"\n","    if iso in special_isos_uroman:\n","        cmd += f\" -l {iso} \"\n","    cmd +=  f\" < {tf.name} > {tf2.name}\"\n","    os.system(cmd)\n","    outtexts = []\n","    with open(tf2.name) as f:\n","        for line in f:\n","            line = \" \".join(line.strip())\n","            line =  re.sub(r\"\\s+\", \" \", line).strip()\n","            outtexts.append(line)\n","    assert len(outtexts) == len(norm_transcripts)\n","    uromans = []\n","    for ot in outtexts:\n","        uromans.append(normalize_uroman(ot))\n","    return uromans\n","\n","\n","\n","@dataclass\n","class Segment:\n","    label: str\n","    start: int\n","    end: int\n","\n","    def __repr__(self):\n","        return f\"{self.label}: [{self.start:5d}, {self.end:5d})\"\n","\n","    @property\n","    def length(self):\n","        return self.end - self.start\n","\n","\n","def merge_repeats(path, idx_to_token_map):\n","    i1, i2 = 0, 0\n","    segments = []\n","    while i1 < len(path):\n","        while i2 < len(path) and path[i1] == path[i2]:\n","            i2 += 1\n","        segments.append(Segment(idx_to_token_map[path[i1]], i1, i2 - 1))\n","        i1 = i2\n","    return segments\n","\n","\n","def time_to_frame(time):\n","    stride_msec = 20\n","    frames_per_sec = 1000 / stride_msec\n","    return int(time * frames_per_sec)\n","\n","\n","\n","def load_model_dict():\n","    model_path_name = \"/tmp/ctc_alignment_mling_uroman_model.pt\"\n","\n","    print(\"Downloading model and dictionary...\")\n","    if os.path.exists(model_path_name):\n","        print(\"Model path already exists. Skipping downloading....\")\n","    else:\n","        torch.hub.download_url_to_file(\n","            \"https://dl.fbaipublicfiles.com/mms/torchaudio/ctc_alignment_mling_uroman/model.pt\",\n","            model_path_name,\n","        )\n","        assert os.path.exists(model_path_name)\n","    state_dict = torch.load(model_path_name, map_location=\"cpu\")\n","\n","    model = wav2vec2_model(\n","        extractor_mode=\"layer_norm\",\n","        extractor_conv_layer_config=[\n","            (512, 10, 5),\n","            (512, 3, 2),\n","            (512, 3, 2),\n","            (512, 3, 2),\n","            (512, 3, 2),\n","            (512, 2, 2),\n","            (512, 2, 2),\n","        ],\n","        extractor_conv_bias=True,\n","        encoder_embed_dim=1024,\n","        encoder_projection_dropout=0.0,\n","        encoder_pos_conv_kernel=128,\n","        encoder_pos_conv_groups=16,\n","        encoder_num_layers=24,\n","        encoder_num_heads=16,\n","        encoder_attention_dropout=0.0,\n","        encoder_ff_interm_features=4096,\n","        encoder_ff_interm_dropout=0.1,\n","        encoder_dropout=0.0,\n","        encoder_layer_norm_first=True,\n","        encoder_layer_drop=0.1,\n","        aux_num_out=31,\n","    )\n","    model.load_state_dict(state_dict)\n","    model.eval()\n","\n","    dict_path_name = \"/tmp/ctc_alignment_mling_uroman_model.dict\"\n","    if os.path.exists(dict_path_name):\n","        print(\"Dictionary path already exists. Skipping downloading....\")\n","    else:\n","        torch.hub.download_url_to_file(\n","            \"https://dl.fbaipublicfiles.com/mms/torchaudio/ctc_alignment_mling_uroman/dictionary.txt\",\n","            dict_path_name,\n","        )\n","        assert os.path.exists(dict_path_name)\n","    dictionary = {}\n","    with open(dict_path_name) as f:\n","        dictionary = {l.strip(): i for i, l in enumerate(f.readlines())}\n","\n","    return model, dictionary\n","\n","def get_spans(tokens, segments):\n","    ltr_idx = 0\n","    tokens_idx = 0\n","    intervals = []\n","    start, end = (0, 0)\n","    sil = \"<blank>\"\n","    for (seg_idx, seg) in enumerate(segments):\n","        if(tokens_idx == len(tokens)):\n","           assert(seg_idx == len(segments) - 1)\n","           assert(seg.label == '<blank>')\n","           continue\n","        cur_token = tokens[tokens_idx].split(' ')\n","        ltr = cur_token[ltr_idx]\n","        if seg.label == \"<blank>\": continue\n","        assert(seg.label == ltr)\n","        if(ltr_idx) == 0: start = seg_idx\n","        if ltr_idx == len(cur_token) - 1:\n","            ltr_idx = 0\n","            tokens_idx += 1\n","            intervals.append((start, seg_idx))\n","            while tokens_idx < len(tokens) and len(tokens[tokens_idx]) == 0:\n","                    intervals.append((seg_idx, seg_idx))\n","                    tokens_idx += 1\n","        else:\n","            ltr_idx += 1\n","    spans = []\n","    for (idx, (start, end)) in enumerate(intervals):\n","        span = segments[start:end + 1]\n","        if start > 0:\n","            prev_seg = segments[start - 1]\n","            if prev_seg.label == sil:\n","                pad_start = prev_seg.start if (idx == 0) else int((prev_seg.start + prev_seg.end)/2)\n","                span = [Segment(sil, pad_start, span[0].start)] + span\n","        if end+1 < len(segments):\n","            next_seg = segments[end+1]\n","            if next_seg.label == sil:\n","                pad_end = next_seg.end if (idx == len(intervals) - 1) else math.floor((next_seg.start + next_seg.end) / 2)\n","                span = span + [Segment(sil, span[-1].end, pad_end)]\n","        spans.append(span)\n","    return spans\n","\n","import torchaudio.functional as F\n","\n","SAMPLING_FREQ = 16000\n","EMISSION_INTERVAL = 30\n","DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","def generate_emissions(model, audio_file):\n","    waveform, _ = torchaudio.load(audio_file)  # waveform: channels X T\n","    waveform = waveform.to(DEVICE)\n","    total_duration = sox.file_info.duration(audio_file)\n","\n","    audio_sf = sox.file_info.sample_rate(audio_file)\n","    assert audio_sf == SAMPLING_FREQ\n","\n","    emissions_arr = []\n","    with torch.inference_mode():\n","        i = 0\n","        while i < total_duration:\n","            segment_start_time, segment_end_time = (i, i + EMISSION_INTERVAL)\n","\n","            context = EMISSION_INTERVAL * 0.1\n","            input_start_time = max(segment_start_time - context, 0)\n","            input_end_time = min(segment_end_time + context, total_duration)\n","            waveform_split = waveform[\n","                :,\n","                int(SAMPLING_FREQ * input_start_time) : int(\n","                    SAMPLING_FREQ * (input_end_time)\n","                ),\n","            ]\n","\n","            model_outs, _ = model(waveform_split)\n","            emissions_ = model_outs[0]\n","            emission_start_frame = time_to_frame(segment_start_time)\n","            emission_end_frame = time_to_frame(segment_end_time)\n","            offset = time_to_frame(input_start_time)\n","\n","            emissions_ = emissions_[\n","                emission_start_frame - offset : emission_end_frame - offset, :\n","            ]\n","            emissions_arr.append(emissions_)\n","            i += EMISSION_INTERVAL\n","\n","    emissions = torch.cat(emissions_arr, dim=0).squeeze()\n","    emissions = torch.log_softmax(emissions, dim=-1)\n","\n","    stride = float(waveform.size(1) * 1000 / emissions.size(0) / SAMPLING_FREQ)\n","\n","    return emissions, stride\n","\n","\n","def get_alignments(\n","    audio_file,\n","    tokens,\n","    model,\n","    dictionary,\n","    use_star,\n","):\n","    # Generate emissions\n","    emissions, stride = generate_emissions(model, audio_file)\n","    T, N = emissions.size()\n","    if use_star:\n","        emissions = torch.cat([emissions, torch.zeros(T, 1).to(DEVICE)], dim=1)\n","\n","    # Force Alignment\n","    if tokens:\n","        token_indices = [dictionary[c] for c in \" \".join(tokens).split(\" \") if c in dictionary]\n","    else:\n","        print(f\"Empty transcript!!!!! for audio file {audio_file}\")\n","        token_indices = []\n","\n","    blank = dictionary[\"<blank>\"]\n","\n","    targets = torch.tensor(token_indices, dtype=torch.int32).to(DEVICE)\n","\n","    input_lengths = torch.tensor(emissions.shape[0]).unsqueeze(-1)\n","    target_lengths = torch.tensor(targets.shape[0]).unsqueeze(-1)\n","    path, _ = F.forced_align(\n","        emissions.unsqueeze(0), targets.unsqueeze(0), input_lengths, target_lengths, blank=blank\n","    )\n","    path = path.squeeze().to(\"cpu\").tolist()\n","\n","    segments = merge_repeats(path, {v: k for k, v in dictionary.items()})\n","    return segments, stride\n","\n","\n","def main(args):\n","    assert not os.path.exists(\n","        args.outdir\n","    ), f\"Error: Output path exists already {args.outdir}\"\n","\n","    transcripts = []\n","    with open(args.text_filepath) as f:\n","        transcripts = [line.strip() for line in f]\n","    print(\"Read {} lines from {}\".format(len(transcripts), args.text_filepath))\n","\n","    norm_transcripts = [text_normalize(line.strip(), args.lang) for line in transcripts]\n","    tokens = get_uroman_tokens(norm_transcripts, args.uroman_path, args.lang)\n","\n","    model, dictionary = load_model_dict()\n","    model = model.to(DEVICE)\n","    if args.use_star:\n","        dictionary[\"<star>\"] = len(dictionary)\n","        tokens = [\"<star>\"] + tokens\n","        transcripts = [\"<star>\"] + transcripts\n","        norm_transcripts = [\"<star>\"] + norm_transcripts\n","\n","    segments, stride = get_alignments(\n","        args.audio_filepath,\n","        tokens,\n","        model,\n","        dictionary,\n","        args.use_star,\n","    )\n","    # Get spans of each line in input text file\n","    spans = get_spans(tokens, segments)\n","\n","    os.makedirs(args.outdir)\n","    with open( f\"{args.outdir}/manifest.json\", \"w\") as f:\n","        for i, t in enumerate(transcripts):\n","            span = spans[i]\n","            seg_start_idx = span[0].start\n","            seg_end_idx = span[-1].end\n","\n","            output_file = f\"{args.outdir}/segment{i}.flac\"\n","\n","            audio_start_sec = seg_start_idx * stride / 1000\n","            audio_end_sec = seg_end_idx * stride / 1000\n","\n","            tfm = sox.Transformer()\n","            tfm.trim(audio_start_sec , audio_end_sec)\n","            tfm.build_file(args.audio_filepath, output_file)\n","\n","            sample = {\n","                \"audio_start_sec\": audio_start_sec,\n","                \"audio_filepath\": str(output_file),\n","                \"duration\": audio_end_sec - audio_start_sec,\n","                \"text\": t,\n","                \"normalized_text\":norm_transcripts[i],\n","                \"uroman_tokens\": tokens[i],\n","            }\n","            f.write(json.dumps(sample) + \"\\n\")\n","\n","    return segments, stride\n","\n","\n","if __name__ == \"__main__\":\n","    parser = argparse.ArgumentParser(description=\"Align and segment long audio files\")\n","    parser.add_argument(\n","        \"-a\", \"--audio_filepath\", type=str, help=\"Path to input audio file\"\n","    )\n","    parser.add_argument(\n","        \"-t\", \"--text_filepath\", type=str, help=\"Path to input text file \"\n","    )\n","    parser.add_argument(\n","        \"-l\", \"--lang\", type=str, default=\"eng\", help=\"ISO code of the language\"\n","    )\n","    parser.add_argument(\n","        \"-u\", \"--uroman_path\", type=str, default=\"eng\", help=\"Location to uroman/bin\"\n","    )\n","    parser.add_argument(\n","        \"-s\",\n","        \"--use_star\",\n","        action=\"store_true\",\n","        help=\"Use star at the start of transcript\",\n","    )\n","    parser.add_argument(\n","        \"-o\",\n","        \"--outdir\",\n","        type=str,\n","        help=\"Output directory to store segmented audio files\",\n","    )\n","    print(\"Using torch version:\", torch.__version__)\n","    print(\"Using torchaudio version:\", torchaudio.__version__)\n","    print(\"Using device: \", DEVICE)\n","    args = parser.parse_args()\n","    main(args)"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"440836a5-d711-4bee-b880-697051b1099d","_uuid":"bcafd693-8594-4551-8bb8-c403059730be","collapsed":false,"execution":{"iopub.execute_input":"2024-09-09T10:08:23.222877Z","iopub.status.busy":"2024-09-09T10:08:23.222581Z","iopub.status.idle":"2024-09-09T10:08:23.278453Z","shell.execute_reply":"2024-09-09T10:08:23.277239Z","shell.execute_reply.started":"2024-09-09T10:08:23.222845Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["%%writefile punctuations.lst\n","\t7355\tINVALID UNICODE\t0x81\n","\t5265\tINVALID UNICODE\t0x90\n","\b\t75\tINVALID UNICODE\t0x8\n","\t31\tINVALID UNICODE\t0x8d\n","\t3\tINVALID UNICODE\t0x94\n","\t2\tINVALID UNICODE\t0x8f\n","\u001a\t2\tINVALID UNICODE\t0x1a\n","\t1\tINVALID UNICODE\t0x9d\n","\t1\tINVALID UNICODE\t0x93\n","\t1\tINVALID UNICODE\t0x92\n","\t8647\tINVALID UNICODE\t0xe295\n","\t6650\tINVALID UNICODE\t0xf21d\n","\t6234\tINVALID UNICODE\t0xf62d\n","\t4815\tINVALID UNICODE\t0xf173\n","\t4789\tINVALID UNICODE\t0xe514\n","\t4409\tINVALID UNICODE\t0xe293\n","\t3881\tINVALID UNICODE\t0xf523\n","\t3788\tINVALID UNICODE\t0xe233\n","\t2448\tINVALID UNICODE\t0xf50f\n","\t2177\tINVALID UNICODE\t0xe232\n","\t1955\tINVALID UNICODE\t0xea7b\n","\t1926\tINVALID UNICODE\t0xf172\n","\t973\tINVALID UNICODE\t0xe290\n","\t972\tINVALID UNICODE\t0xf519\n","\t661\tINVALID UNICODE\t0xe292\n","\t591\tINVALID UNICODE\t0xe328\n","\t509\tINVALID UNICODE\t0xe2fa\n","\t458\tINVALID UNICODE\t0xe234\n","\t446\tINVALID UNICODE\t0xe043\n","\t419\tINVALID UNICODE\t0xe040\n","\t399\tINVALID UNICODE\t0xe2fb\n","\t387\tINVALID UNICODE\t0xe32b\n","\t381\tINVALID UNICODE\t0xe236\n","\t374\tINVALID UNICODE\t0xf511\n","\t314\tINVALID UNICODE\t0xe517\n","\t296\tINVALID UNICODE\t0xe2fe\n","\t293\tINVALID UNICODE\t0xe492\n","\t291\tINVALID UNICODE\t0xf52d\n","\t289\tINVALID UNICODE\t0xe2fc\n","\t195\tINVALID UNICODE\t0xf521\n","\t190\tINVALID UNICODE\t0xe516\n","\t182\tINVALID UNICODE\t0xe041\n","\t178\tINVALID UNICODE\t0xf529\n","\t113\tINVALID UNICODE\t0xe2f9\n","\t87\tINVALID UNICODE\t0xe2d9\n","\t78\tINVALID UNICODE\t0xe32a\n","\t76\tINVALID UNICODE\t0xe291\n","\t74\tINVALID UNICODE\t0xe296\n","\t66\tINVALID UNICODE\t0xe518\n","\t52\tINVALID UNICODE\t0xe32c\n","\t46\tINVALID UNICODE\t0xe2db\n","\t41\tINVALID UNICODE\t0xe231\n","\t34\tINVALID UNICODE\t0xf522\n","\t33\tINVALID UNICODE\t0xf518\n","\t32\tINVALID UNICODE\t0xf513\n","\t27\tINVALID UNICODE\t0xe32d\n","\t25\tINVALID UNICODE\t0xe32e\n","\t23\tINVALID UNICODE\t0xe06b\n","\t15\tINVALID UNICODE\t0xea01\n","\t12\tINVALID UNICODE\t0xe294\n","\t11\tINVALID UNICODE\t0xe203\n","\t8\tINVALID UNICODE\t0xf218\n","\t7\tINVALID UNICODE\t0xe070\n","\t7\tINVALID UNICODE\t0xe013\n","\t5\tINVALID UNICODE\t0xe2de\n","\t4\tINVALID UNICODE\t0xe493\n","\t3\tINVALID UNICODE\t0xf7e8\n","\t3\tINVALID UNICODE\t0xf7d0\n","\t3\tINVALID UNICODE\t0xe313\n","\t2\tINVALID UNICODE\t0xe329\n","\t2\tINVALID UNICODE\t0xe06d\n","\t2\tINVALID UNICODE\t0xe003\n","\t1\tINVALID UNICODE\t0xf50e\n","\t1\tINVALID UNICODE\t0xf171\n","\t1\tINVALID UNICODE\t0xe01d\n","⁯\t71\tNOMINAL DIGIT SHAPES\t0x206f\n","⁠\t3\tWORD JOINER\t0x2060\n","―\t126545\tHORIZONTAL BAR\t0x2015\n","־\t1028\tHEBREW PUNCTUATION MAQAF\t0x5be\n",")\t98429\tRIGHT PARENTHESIS\t0x29\n","]\t27108\tRIGHT SQUARE BRACKET\t0x5d\n","⌋\t1567\tRIGHT FLOOR\t0x230b\n","〕\t97\tRIGHT TORTOISE SHELL BRACKET\t0x3015\n","】\t36\tRIGHT BLACK LENTICULAR BRACKET\t0x3011\n","﴾\t14\tORNATE LEFT PARENTHESIS\t0xfd3e\n","&\t170517\tAMPERSAND\t0x26\n","།\t106330\tTIBETAN MARK SHAD\t0xf0d\n","።\t90203\tETHIOPIC FULL STOP\t0x1362\n","፥\t60484\tETHIOPIC COLON\t0x1365\n","༌\t60464\tTIBETAN MARK DELIMITER TSHEG BSTAR\t0xf0c\n","။\t51567\tMYANMAR SIGN SECTION\t0x104b\n","/\t46929\tSOLIDUS\t0x2f\n","၊\t38042\tMYANMAR SIGN LITTLE SECTION\t0x104a\n","·\t37985\tMIDDLE DOT\t0xb7\n","‸\t36310\tCARET\t0x2038\n","*\t34793\tASTERISK\t0x2a\n","۔\t32432\tARABIC FULL STOP\t0x6d4\n","፤\t31906\tETHIOPIC SEMICOLON\t0x1364\n","၏\t21519\tMYANMAR SYMBOL GENITIVE\t0x104f\n","។\t20834\tKHMER SIGN KHAN\t0x17d4\n","꓾\t15773\tLISU PUNCTUATION COMMA\t0xa4fe\n","᙮\t13473\tCANADIAN SYLLABICS FULL STOP\t0x166e\n","꤯\t12892\tKAYAH LI SIGN SHYA\t0xa92f\n","⵰\t11478\tTIFINAGH SEPARATOR MARK\t0x2d70\n","꓿\t11118\tLISU PUNCTUATION FULL STOP\t0xa4ff\n","॥\t10763\tDEVANAGARI DOUBLE DANDA\t0x965\n","؞\t10403\tARABIC TRIPLE DOT PUNCTUATION MARK\t0x61e\n","၍\t8936\tMYANMAR SYMBOL COMPLETED\t0x104d\n","·\t8431\tGREEK ANO TELEIA\t0x387\n","†\t7477\tDAGGER\t0x2020\n","၌\t6632\tMYANMAR SYMBOL LOCATIVE\t0x104c\n","፣\t5719\tETHIOPIC COMMA\t0x1363\n","៖\t5528\tKHMER SIGN CAMNUC PII KUUH\t0x17d6\n","꤮\t4791\tKAYAH LI SIGN CWI\t0xa92e\n","※\t3439\tREFERENCE MARK\t0x203b\n","፦\t2727\tETHIOPIC PREFACE COLON\t0x1366\n","•\t1749\tBULLET\t0x2022\n","¶\t1507\tPILCROW SIGN\t0xb6\n","၎\t1386\tMYANMAR SYMBOL AFOREMENTIONED\t0x104e\n","﹖\t1224\tSMALL QUESTION MARK\t0xfe56\n",";\t975\tGREEK QUESTION MARK\t0x37e\n","…\t827\tHORIZONTAL ELLIPSIS\t0x2026\n","%\t617\tPERCENT SIGN\t0x25\n","・\t468\tKATAKANA MIDDLE DOT\t0x30fb\n","༎\t306\tTIBETAN MARK NYIS SHAD\t0xf0e\n","‡\t140\tDOUBLE DAGGER\t0x2021\n","#\t137\tNUMBER SIGN\t0x23\n","@\t125\tCOMMERCIAL AT\t0x40\n","፡\t121\tETHIOPIC WORDSPACE\t0x1361\n","៚\t55\tKHMER SIGN KOOMUUT\t0x17da\n","៕\t49\tKHMER SIGN BARIYOOSAN\t0x17d5\n","﹐\t10\tSMALL COMMA\t0xfe50\n","༅\t6\tTIBETAN MARK CLOSING YIG MGO SGAB MA\t0xf05\n","༄\t6\tTIBETAN MARK INITIAL YIG MGO MDUN MA\t0xf04\n","．\t2\tFULLWIDTH FULL STOP\t0xff0e\n","﹗\t2\tSMALL EXCLAMATION MARK\t0xfe57\n","﹕\t2\tSMALL COLON\t0xfe55\n","‰\t2\tPER MILLE SIGN\t0x2030\n","･\t1\tHALFWIDTH KATAKANA MIDDLE DOT\t0xff65\n","(\t98504\tLEFT PARENTHESIS\t0x28\n","[\t27245\tLEFT SQUARE BRACKET\t0x5b\n","⌊\t1567\tLEFT FLOOR\t0x230a\n","〔\t95\tLEFT TORTOISE SHELL BRACKET\t0x3014\n","【\t36\tLEFT BLACK LENTICULAR BRACKET\t0x3010\n","﴿\t14\tORNATE RIGHT PARENTHESIS\t0xfd3f\n","_\t4851\tLOW LINE\t0x5f\n","$\t72\tDOLLAR SIGN\t0x24\n","€\t14\tEURO SIGN\t0x20ac\n","£\t2\tPOUND SIGN\t0xa3\n","~\t27462\tTILDE\t0x7e\n","=\t11450\tEQUALS SIGN\t0x3d\n","|\t8430\tVERTICAL LINE\t0x7c\n","−\t3971\tMINUS SIGN\t0x2212\n","≫\t1904\tMUCH GREATER-THAN\t0x226b\n","≪\t1903\tMUCH LESS-THAN\t0x226a\n","+\t1450\tPLUS SIGN\t0x2b\n","＜\t345\tFULLWIDTH LESS-THAN SIGN\t0xff1c\n","＞\t344\tFULLWIDTH GREATER-THAN SIGN\t0xff1e\n","¬\t5\tNOT SIGN\t0xac\n","×\t4\tMULTIPLICATION SIGN\t0xd7\n","→\t2\tRIGHTWARDS ARROW\t0x2192\n","᙭\t537\tCANADIAN SYLLABICS CHI SIGN\t0x166d\n","°\t499\tDEGREE SIGN\t0xb0\n","႟\t421\tMYANMAR SYMBOL SHAN EXCLAMATION\t0x109f\n","�\t192\tREPLACEMENT CHARACTER\t0xfffd\n","⌟\t54\tBOTTOM RIGHT CORNER\t0x231f\n","⌞\t54\tBOTTOM LEFT CORNER\t0x231e\n","©\t2\tCOPYRIGHT SIGN\t0xa9\n"," \t40\tNARROW NO-BREAK SPACE\t0x202f\n"," \t1\tSIX-PER-EM SPACE\t0x2006\n","˜\t40261\tSMALL TILDE\t0x2dc\n","^\t6469\tCIRCUMFLEX ACCENT\t0x5e\n","¯\t20\tMACRON\t0xaf\n","ˇ\t191442\tCARON\t0x2c7\n","ⁿ\t38144\tSUPERSCRIPT LATIN SMALL LETTER N\t0x207f\n","ـ\t9440\tARABIC TATWEEL\t0x640\n","ๆ\t6766\tTHAI CHARACTER MAIYAMOK\t0xe46\n","ៗ\t3310\tKHMER SIGN LEK TOO\t0x17d7\n","々\t678\tIDEOGRAPHIC ITERATION MARK\t0x3005\n","ໆ\t430\tLAO KO LA\t0xec6\n","ー\t319\tKATAKANA-HIRAGANA PROLONGED SOUND MARK\t0x30fc\n","ⁱ\t137\tSUPERSCRIPT LATIN SMALL LETTER I\t0x2071\n","৷\t11056\tBENGALI CURRENCY NUMERATOR FOUR\t0x9f7\n","⅓\t26\tVULGAR FRACTION ONE THIRD\t0x2153\n","½\t26\tVULGAR FRACTION ONE HALF\t0xbd\n","¼\t4\tVULGAR FRACTION ONE QUARTER\t0xbc\n","⅟\t1\tFRACTION NUMERATOR ONE\t0x215f\n","⁄\t57\tFRACTION SLASH\t0x2044"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"607fec99-1acb-456a-a068-b639b24f93e3","_uuid":"eab6f63a-a0f6-41c3-be0f-0a5378d61e75","collapsed":false,"execution":{"iopub.execute_input":"2024-09-09T10:08:23.280495Z","iopub.status.busy":"2024-09-09T10:08:23.280084Z","iopub.status.idle":"2024-09-09T10:08:23.292863Z","shell.execute_reply":"2024-09-09T10:08:23.291574Z","shell.execute_reply.started":"2024-09-09T10:08:23.280445Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["%cd /kaggle/working/uroman"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6276b8be-02c8-4f41-8d1e-d788cd326b59","_uuid":"a75a4bda-a881-4944-b059-0d15c93d4ebf","collapsed":false,"execution":{"iopub.execute_input":"2024-09-09T10:08:23.294892Z","iopub.status.busy":"2024-09-09T10:08:23.294442Z","iopub.status.idle":"2024-09-09T10:08:36.463588Z","shell.execute_reply":"2024-09-09T10:08:36.462488Z","shell.execute_reply.started":"2024-09-09T10:08:23.294837Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["!pip install -r requirements.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"422bf302-fb1a-46c5-900a-41b68d0bd3ac","_uuid":"a53a0814-55e6-4677-98a0-64619d6d60f3","collapsed":false,"execution":{"iopub.execute_input":"2024-09-09T10:08:36.465195Z","iopub.status.busy":"2024-09-09T10:08:36.464881Z","iopub.status.idle":"2024-09-09T10:08:52.325031Z","shell.execute_reply":"2024-09-09T10:08:52.323872Z","shell.execute_reply.started":"2024-09-09T10:08:36.465160Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["!pip install ffmpeg"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"794085a6-83fb-46ec-adf5-b150984d2f1d","_uuid":"b699a22c-4879-4e7b-9ffd-ad14ccf0325a","collapsed":false,"execution":{"iopub.execute_input":"2024-08-28T20:53:32.158275Z","iopub.status.busy":"2024-08-28T20:53:32.157871Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["import os\n","import torch\n","from pathlib import Path\n","#give required paths\n","audio_dir = \n","text_dir = \n","uroman_path = \n","output_base_dir =\n","output_temp =\n","audio_files = [f for f in os.listdir(audio_dir) if f.endswith(\".wav\")]\n","\n","for audio_file in audio_files:\n","    torch.cuda.empty_cache()\n","    f = audio_file.split(\"-\")\n","    name = #alter name as required\n","    try:\n","        text_file = os.path.join(text_dir, f\"{name}.txt\")\n","        output_dir = os.path.join(output_temp, name)\n","        if not os.path.exists(text_file):\n","            print(f\"Text file not found for: {audio_file}, skipping...\")\n","            continue\n","        #os.makedirs(output_dir, exist_ok=True)\n","        #command = f'ffmpeg -i \"{os.path.join(audio_dir, audio_file)}\" -ar 16000 -ac 1 \"{os.path.join(output_temp, audio_file)}\"'\n","        #os.system(command)\n","        command = f'python \"/kaggle/working/align.py\" -a \"{os.path.join(audio_dir, audio_file)}\" -t \"{text_file}\" -l mni -u \"{uroman_path}\" -o \"{output_dir}/\"'\n","        os.system(command)\n","       #print(f\"Processed: {audio_file} with {text_file}\")\n","        with open(f'{output_dir}/manifest.json','r') as f:\n","            with open(f'{output_base_dir}/{name}.json','w') as new_f:\n","                data = f.read()\n","                new_f.write(data)\n","        #print(\"Manifest.json Saved\")\n","        command = f'rm -r {output_dir}'\n","        os.system(command)\n","        #command = f'rm {os.path.join(output_temp, audio_file)}'\n","        #os.system(command)\n","        #print(\"Removed files and folder\")\n","    except:\n","        print(\"Could not find \", text_file)"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f1d3c0b9-5372-4b34-a2eb-11522a4eec73","_uuid":"7197d2e5-6efb-4b6b-95cd-100426eca998","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":5607507,"sourceId":9266294,"sourceType":"datasetVersion"},{"datasetId":5608502,"sourceId":9267888,"sourceType":"datasetVersion"},{"datasetId":5609438,"sourceId":9269366,"sourceType":"datasetVersion"}],"dockerImageVersionId":30762,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
